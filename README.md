# AI Engineering: Data ETL Pipeline with LLMs

### 1. Project Objective (Business Context)

The objective of this project is to simulate an order from a pharmaceutical company: to extract variables of interest dynamically from a corpus of unstructured documents (PDFs of *papers* pharmaceuticals).

The final deliverable is a single `.csv` table where each row represents a document and each column an extracted interest variable. If a document does not contain a variable, a null value (`null`) must be imputed.

---

### 2. Technical Challenge and Engineering Solution

Performing this task in an environment with limited resources (such as Google Colab T4) presents three major AI engineering challenges:

1.  **Memory (OOM):** Powerful LLMs (7B+ params) do not fit in GPUs of <16GB VRAM.
2.  **Efficiency (Latency):** Processing N documents sequentially (one by one) is extremely slow.
3.  **Reliability (Parsing):** LLMs are "chatty" and rarely return perfect JSON, which causes parsing failures.

This notebook implements an **optimized AI pipeline** to solve these problems:

* **Model:** `mistralai/Mistral-7B-Instruct-v0.3` (a high-performance 7B model).
* **Memory Optimization:** **4-bit (NF4)** quantization is used with `bitsandbytes` to load the 7B model on ~5GB of VRAM, avoiding OOM errors.
* **Batching:** The code was refactored to go from N sequential calls to only **2 calls to the GPU** (one to discover variables and another to extract data), multiplying efficiency.
* **Robust Parsing:** A `robust_json_parser` was implemented that "hunts" the JSON block within the LLM response, ignoring the extra text to avoid failures.
* **Data Cleaning:** `difflib` is used to find and merge columns with similar names (e.g. "placebo effect" and "placebo\_effect") dynamically generated by AI.

---

### Project Structure

* `README.md`: This file.
* `requirements.txt`: Python dependencies.
* `notebooks/AI_ETL.ipynb`: The main Jupyter Notebook containing all code.
* `data/raw/`: Place your source `.pdf` files here.
* `data/processed/`: Intermediate `.txt` files generated by the notebook.
* `data/output/`: The final `dynamic_table_final.csv` result.

---

### Quickstart

1.  **Clone** this repository.
2.  **Create Environment:** Create a Python 3.10+ virtual environment and activate it.
3.  **Install Deps:** Install dependencies. For GPU (recommended):
    ```sh
    pip install -r requirements.txt
    ```
4.  **Add Data:** Place your source `.pdf` files into the `data/raw/` directory.
5.  **Run Pipeline:** Open and run all cells in `notebooks/AI_ETL.ipynb`.
    * The notebook first runs the PDF-to-text conversion, saving `.txt` files to `data/processed/`.
    * It then runs the two-stage batch LLM pipeline to extract all data.

---

### Performance & Output

* **Result:** The pipeline successfully processed **6 documents** in **561 seconds** (9.3 minutes) on a T4 GPU.
* **Output:** The final table generated dynamically contained **58 unique columns** extracted from the source texts.
* **Final CSV:** The result is saved to `data/output/dynamic_table_final.csv`.

### Key Functions

* `pdf_to_text_worker`: Converts PDFs from `data/raw` to text files in `data/processed`.
* `generate_batch`: A highly efficient function that sends batches of prompts to the GPU, avoiding sequential loops.
* `extract_variables_batch`: First major GPU call to discover all unique column names from all documents.
* `extract_data_batch`: Second major GPU call to "fill in" the table with data for all documents.
* `robust_json_parser`: "Hunts" for the JSON block in a "chatty" LLM response to prevent parsing errors.
* `normalize_key` & `difflib`: Cleans and merges similar column names (e.g., "placebo effect" vs. "placebo\_effect").